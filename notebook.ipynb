{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\rdv_finder\\\\test.html'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "cwd = os.getcwd()\n",
    "cwd + \"\\\\test.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods dev chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from PIL import Image\n",
    "\n",
    "def setup_driver():\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/89.0\",\n",
    "    ]\n",
    "    window_sizes = [\"1920x1080\", \"1366x768\", \"1280x720\"]\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(f'user-agent={random.choice(user_agents)}')\n",
    "    options.add_argument(f'--window-size={random.choice(window_sizes)}')\n",
    "    # options.add_argument('--headless')  # Run in headless mode if required\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def get_html(url):\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait 10 seconds so that the content can be fully loaded\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        wait.until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "        # Mimicking human behavior by waiting a random time\n",
    "        time.sleep(random.randint(2,5))\n",
    "        html = driver.page_source\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return html\n",
    "\n",
    "def get_captcha(html, id):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    captcha_element = soup.find(id=id)\n",
    "    pretty_html = soup.prettify()\n",
    "    captcha_url = captcha_element['src']\n",
    "    captcha_url = captcha_url[5:]\n",
    "    image_response = requests.get(captcha_url)\n",
    "    if image_response.status_code == 200:\n",
    "        with open(\"downloaded_image.png\", \"wb\") as file:\n",
    "            file.write(image_response.content)\n",
    "        print(\"Image saved as downloaded_image.png\")\n",
    "    else:\n",
    "        print(\"Failed to download the image\")\n",
    "    # Specify the path and name of your HTML file\n",
    "    file_path = cwd + '\\\\test.png'\n",
    "    # Open a file at the specified path in 'write' mode\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(pretty_html)  # Write the JavaScript code to the file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_captcha(url):\n",
    "    cwd = os.getcwd()\n",
    "    screenshot_path = cwd + \"\\\\full_screenshot.png\"\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait 10 seconds so that the content can be fully loaded\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        wait.until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "        # Mimicking human behavior by waiting a random time\n",
    "        time.sleep(random.randint(2,5))\n",
    "        # Scroll to a specific position (e.g., 500 pixels down)\n",
    "        scroll_position = 1000  # Replace with the desired vertical scroll position in pixels\n",
    "        driver.execute_script(f\"window.scrollTo(0, {scroll_position});\")\n",
    "        time.sleep(2)  # Adjust sleep time if needed for page loading\n",
    "        driver.save_screenshot(\"full_screenshot.png\")\n",
    "        with Image.open(screenshot_path) as img:\n",
    "            width, height = img.size\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.rdv-prefecture.interieur.gouv.fr/rdvpref/reservation/demarche/4443/cgu/'\n",
    "cwd = os.getcwd()\n",
    "screenshot_path = cwd + \"\\\\full_screenshot.png\"\n",
    "driver = setup_driver()\n",
    "try:\n",
    "    driver.get(url)\n",
    "    # Wait 10 seconds so that the content can be fully loaded\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    wait.until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "    # Mimicking human behavior by waiting a random time\n",
    "    time.sleep(random.randint(2,5))\n",
    "    # Scroll to a specific position (e.g., 500 pixels down)\n",
    "    captcha_image = driver.find_element(By.ID, \"captchaFR_CaptchaImage\")\n",
    "    captcha_image.screenshot(\"captcha.png\")\n",
    "\n",
    "    # scroll_position = 1000  # Replace with the desired vertical scroll position in pixels\n",
    "    # driver.execute_script(f\"window.scrollTo(0, {scroll_position});\")\n",
    "    # time.sleep(2)  # Adjust sleep time if needed for page loading\n",
    "    # driver.save_screenshot(\"full_screenshot.png\")\n",
    "    # with Image.open(screenshot_path) as img:\n",
    "    #     width, height = img.size\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test get_captcha function\n",
    "url = 'https://www.rdv-prefecture.interieur.gouv.fr/rdvpref/reservation/demarche/4443/cgu/'\n",
    "\n",
    "width, height = get_captcha(url)\n",
    "\n",
    "cw, ch = width/2, height/2\n",
    "r = 0.15\n",
    "\n",
    "with Image.open(\"full_screenshot.png\") as full_img:\n",
    "    fine_crop = 140 # Fine-tune the cropping by adjusting this value\n",
    "    fine_crop_right = 100 # Fine-tune the cropping by adjusting this value\n",
    "    # (left, top, right, bottom)\n",
    "    element_screenshot = full_img.crop((cw-r*width, ch-r*height+fine_crop, cw+r*width-fine_crop_right, ch+r*height-fine_crop))\n",
    "    element_screenshot.save(\"element_screenshot.png\")  # Save the cropped image\n",
    "\n",
    "print(\"JavaScript file written successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get captchas loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JavaScript file written successfully 1 times!\n",
      "JavaScript file written successfully 2 times!\n"
     ]
    }
   ],
   "source": [
    "num_captchas = 2\n",
    "for i in range(num_captchas):\n",
    "    url = 'https://www.rdv-prefecture.interieur.gouv.fr/rdvpref/reservation/demarche/4443/cgu/'\n",
    "\n",
    "    width, height = get_captcha(url)\n",
    "    cw, ch = width/2, height/2\n",
    "    r = 0.2\n",
    "\n",
    "    with Image.open(\"full_screenshot.png\") as full_img:\n",
    "        fine_crop = 120 # Fine-tune the cropping by adjusting this value\n",
    "        fine_crop_right = 100 # Fine-tune the cropping by adjusting this value\n",
    "        # (left, top, right, bottom)\n",
    "        element_screenshot = full_img.crop((cw-r*width, ch-r*height, cw+r*width-fine_crop_right, ch+r*height))\n",
    "        element_screenshot.save(f\"captchas/screenshot_{i}.png\")  # Save the cropped image\n",
    "\n",
    "    print(f\"JavaScript file written successfully {i+1} times!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.2\n",
    "with Image.open(\"full_screenshot.png\") as full_img:\n",
    "        fine_crop = 120 # Fine-tune the cropping by adjusting this value\n",
    "        fine_crop_right = 100 # Fine-tune the cropping by adjusting this value\n",
    "        # (left, top, right, bottom)\n",
    "        element_screenshot = full_img.crop((cw-r*width+30, ch-r*height+110, cw+r*width-80, ch+r*height-150))\n",
    "        element_screenshot.save(f\"screenshot.png\")  # Save the cropped image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\rdv_finder\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like naver-clova-ix/donut-base-finetuned-docvqa is not the path to a directory containing a file named preprocessor_config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32me:\\rdv_finder\\venv\\Lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\rdv_finder\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\rdv_finder\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\rdv_finder\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32me:\\rdv_finder\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1478\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_files_only:\n\u001b[1;32m-> 1478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[0;32m   1479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m hf.co look-ups and downloads online, set \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1481\u001b[0m     )\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1483\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load Donut model and processor\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaver-clova-ix/donut-base-finetuned-docvqa\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# or another fine-tuned model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mDonutProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify a cache directory\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Force re-download of the model files\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure it tries to download if not available locally)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Move model to device (CPU or GPU)\u001b[39;00m\n",
      "File \u001b[1;32me:\\rdv_finder\\venv\\Lib\\site-packages\\transformers\\processing_utils.py:944\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    942\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[1;32m--> 944\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    945\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\rdv_finder\\venv\\Lib\\site-packages\\transformers\\processing_utils.py:990\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    988\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[1;32m--> 990\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattribute_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32me:\\rdv_finder\\venv\\Lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:415\u001b[0m, in \u001b[0;36mAutoImageProcessor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    413\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[43mImageProcessingMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m image_processor_class \u001b[38;5;241m=\u001b[39m config_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_processor_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    417\u001b[0m image_processor_auto_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\rdv_finder\\venv\\Lib\\site-packages\\transformers\\image_processing_base.py:335\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    332\u001b[0m image_processor_file \u001b[38;5;241m=\u001b[39m IMAGE_PROCESSOR_NAME\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m     resolved_image_processor_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_processor_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32me:\\rdv_finder\\venv\\Lib\\site-packages\\transformers\\utils\\hub.py:446\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    441\u001b[0m         resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_missing_entries\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors\n\u001b[0;32m    444\u001b[0m     ):\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load this file, couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find it in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m cached files and it looks like \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not the path to a directory containing a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[1;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like naver-clova-ix/donut-base-finetuned-docvqa is not the path to a directory containing a file named preprocessor_config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load Donut model and processor\n",
    "model_name = \"naver-clova-ix/donut-base-finetuned-docvqa\"  # or another fine-tuned model\n",
    "processor = DonutProcessor.from_pretrained(model_name,\n",
    "                                           cache_dir=\"./cache\",  # Specify a cache directory\n",
    "                                           force_download=False,  # Force re-download of the model files\n",
    "                                           local_files_only=True)  # Ensure it tries to download if not available locally)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2560, 1920])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load an image from file\n",
    "#image_path = \"captchas/screenshot_1.png\"\n",
    "image_path = \"captchas/simple.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Preprocess the image for the model\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "pixel_values = pixel_values.to(device)\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized Text: m m m m m m\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(pixel_values, max_length=8, min_length=6, num_beams=5)\n",
    "\n",
    "# Decode the generated text\n",
    "recognized_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Recognized Text: {recognized_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This actually works very well!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\rdv_finder\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\march\\.cache\\huggingface\\hub\\models--microsoft--trocr-base-printed. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.46.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained TrOCR model and processor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized Text: PUSKIT\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image = Image.open(\"captchas/screenshot_1.png\").convert(\"RGB\")\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# Perform OCR\n",
    "outputs = model.generate(pixel_values)\n",
    "recognized_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(\"Recognized Text:\", recognized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.46.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Load the processor and model\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "\n",
    "# # Get the tokenizer\n",
    "# tokenizer = processor.tokenizer\n",
    "\n",
    "# # Modify the vocabulary to include only numbers and letters\n",
    "# allowed_tokens = [token for token in tokenizer.get_vocab() if token.isalnum()]\n",
    "# tokenizer.add_tokens(allowed_tokens)\n",
    "\n",
    "# # Set the new tokenizer back to the processor\n",
    "# processor.tokenizer = tokenizer\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "for i in range(1, 21):\n",
    "    image_pathe = f\"captchas/screenshot_{i}.png\"\n",
    "    image = cv2.imread(image_pathe, cv2.IMREAD_GRAYSCALE)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    image = clahe.apply(image)\n",
    "    kernel_size = [2, 2]\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size)\n",
    "    cleaned = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "    cv2.imwrite(f\"captchas/cleaned_{i}.png\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized Text 1: NSA3VE\n",
      "Recognized Text 2: X40N6X\n",
      "Recognized Text 3: UM9FK\n",
      "Recognized Text 4: 06514\n",
      "Recognized Text 5: 9STURK\n",
      "Recognized Text 6: DULEN431\n",
      "Recognized Text 7: SYCASHED\n",
      "Recognized Text 8: 4E09MM3\n",
      "Recognized Text 9: S165MMUM\n",
      "Recognized Text 10: TW406\n",
      "Recognized Text 11: BM7M6X\n",
      "Recognized Text 12: BSBA3N\n",
      "Recognized Text 13: AAXANS\n",
      "Recognized Text 14: 46SXNKY\n",
      "Recognized Text 15: T159KAP53\n",
      "Recognized Text 16: AVIS35X\n",
      "Recognized Text 17: 83984RA6\n",
      "Recognized Text 18: 3STRSHT3\n",
      "Recognized Text 19: M995MM\n",
      "Recognized Text 20: 54MMBXB\n",
      "Failed to pass the CAPTCHA!\n",
      "Failed to enter the confirmation page!\n"
     ]
    }
   ],
   "source": [
    "max_attempts = 20\n",
    "timeout = 10\n",
    "url = 'https://www.rdv-prefecture.interieur.gouv.fr/rdvpref/reservation/demarche/4443/cgu/'\n",
    "cwd = os.getcwd()\n",
    "screenshot_path = cwd + \"\\\\full_screenshot.png\"\n",
    "driver = setup_driver()\n",
    "try:\n",
    "    count = 0\n",
    "    driver.get(url)\n",
    "    # Wait 10 seconds so that the content can be fully loaded\n",
    "    wait = WebDriverWait(driver, timeout)\n",
    "    wait.until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "    # Mimicking human behavior by waiting a random time\n",
    "    time.sleep(random.randint(2,5))\n",
    "    # Scroll to a specific position (e.g., 500 pixels down)\n",
    "    # scroll_position = 1000  # Replace with the desired vertical scroll position in pixels\n",
    "    # driver.execute_script(f\"window.scrollTo(0, {scroll_position});\")\n",
    "    time.sleep(2)  # Adjust sleep time if needed for page loading\n",
    "    current_url = driver.current_url\n",
    "    expected_url = \"https://www.rdv-prefecture.interieur.gouv.fr/rdvpref/reservation/demarche/4443/creneau/\"\n",
    "    while current_url != expected_url and count < max_attempts:\n",
    "        # driver.save_screenshot(\"full_screenshot.png\")\n",
    "        # with Image.open(screenshot_path) as img:\n",
    "        #     width, height = img.size\n",
    "        # cw, ch = width/2, height/2\n",
    "        # r = 0.2\n",
    "        # with Image.open(\"full_screenshot.png\") as full_img:\n",
    "        #     # (left, top, right, bottom)\n",
    "        #     coords = (cw-r*width+30, ch-r*height+110, cw+r*width-80, ch+r*height-150)\n",
    "        #     element_screenshot = full_img.crop(coords)\n",
    "        #     element_screenshot.save(f\"captchas/screenshot_{count+1}.png\")  # Save the cropped image\n",
    "        image_path = f\"captchas/screenshot_{count+1}.png\"\n",
    "        captcha_image = WebDriverWait(driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.ID, \"captchaFR_CaptchaImage\"))\n",
    "        )\n",
    "        captcha_image = driver.find_element(By.ID, \"captchaFR_CaptchaImage\")\n",
    "        captcha_image.screenshot(image_path)\n",
    "        ###########################################\n",
    "        captcha_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        captcha_image = clahe.apply(captcha_image)\n",
    "        kernel_size = [2, 2]\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size)\n",
    "        captcha_image = cv2.morphologyEx(captcha_image, cv2.MORPH_CLOSE, kernel)\n",
    "        cv2.imwrite(f\"captchas/cleaned_{i}.png\", captcha_image)\n",
    "        ###########################################\n",
    "        cleaned_path = f\"captchas/cleaned_{count+1}.png\"\n",
    "        image = Image.open(cleaned_path).convert(\"RGB\")\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(pixel_values, max_length=9, min_length=6, num_beams=5)\n",
    "        decoded_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        recognized_text = re.sub(r\"[^A-Z0-9]\", \"\", decoded_text)\n",
    "        print(f\"Recognized Text {count+1}: {recognized_text}\")\n",
    "        captcha_input = driver.find_element(By.ID, \"captchaFormulaireExtInput\")\n",
    "        captcha_input.send_keys(recognized_text)\n",
    "        button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[.//span[text()='Suivant']]\"))\n",
    "        )\n",
    "        button.click()\n",
    "        # Wait until the next page is loaded\n",
    "        wait.until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "        current_url = driver.current_url\n",
    "        count += 1\n",
    "    if count >= max_attempts:\n",
    "        print(\"Failed to pass the CAPTCHA!\")\n",
    "    else:\n",
    "        print(\"Successfully passed the CAPTCHA!\")\n",
    "    # Wait for the button to be present in the DOM\n",
    "    button = WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"q-btn--actionable\"))  # Replace with the correct locator\n",
    "    )\n",
    "    button.click()  # Perform click action\n",
    "    expected_url = \"https://www.rdv-prefecture.interieur.gouv.fr/rdvpref/reservation/demarche/4443/confirmation/\"\n",
    "    current_url = driver.current_url\n",
    "    if current_url == expected_url:\n",
    "        print(\"Successfully entered the confirmation page!\")\n",
    "    else:\n",
    "        print(\"Failed to enter the confirmation page!\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# cw, ch = width/2, height/2\n",
    "# r = 0.2\n",
    "\n",
    "# with Image.open(\"full_screenshot.png\") as full_img:\n",
    "#     # (left, top, right, bottom)\n",
    "#     coords = (cw-r*width+30, ch-r*height+110, cw+r*width-80, ch+r*height-150)\n",
    "#     element_screenshot = full_img.crop(coords)\n",
    "#     element_screenshot.save(f\"captchas/screenshot_{i+1}.png\")  # Save the cropped image\n",
    "# image_path = f\"captchas/screenshot_{i+1}.png\"\n",
    "# image = Image.open(image_path).convert(\"RGB\")\n",
    "# pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "# pixel_values = pixel_values.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(pixel_values, max_length=8, min_length=6, num_beams=5)\n",
    "\n",
    "# recognized_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(f\"Recognized Text {i+1}: {recognized_text}\")\n",
    "\n",
    "# print(f\"JavaScript file written {i+1} times!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
